# -*- coding: utf-8 -*-
"""cs599_lab2.ipynb

Automatically generated by Colab.
"""

import numpy as np
import tensorflow as tf
import random
from tensorflow.keras.datasets import mnist


def name_to_seed(name="Nicole"):
    return sum(ord(c) for c in name)  # Convert name to a unique seed

# Generate seed from your name
seed = name_to_seed("Nicole")

# Set seed for reproducibility
random.seed(seed)            # Python
np.random.seed(seed)         # NumPy
tf.random.set_seed(seed)     # TensorFlow

print(f"Using seed: {seed}")


# Constants
NUM_TASKS = 10  # Number of tasks
INPUT_SHAPE = 784  # Flattened MNIST image size

# Load MNIST dataset
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the images: pixel values in [0,1]
train_images = train_images.astype(np.float32) / 255.0
test_images = test_images.astype(np.float32) / 255.0

# Reshape images into 1D vectors of size 784
train_images = train_images.reshape(-1, INPUT_SHAPE)
test_images = test_images.reshape(-1, INPUT_SHAPE)

# Generate a list of tasks: each task is a random permutation of the 784 input pixels
task_train_images = []
task_test_images = []
permutations = []

for task_index in range(NUM_TASKS):
    permutation = np.random.permutation(INPUT_SHAPE)
    permutations.append(permutation)

    permuted_train = train_images[:, permutation]
    permuted_test = test_images[:, permutation]

    task_train_images.append(permuted_train)
    task_test_images.append(permuted_test)

# Convert to numpy arrays for efficient access
task_train_images = np.array(task_train_images)
task_test_images = np.array(task_test_images)
task_train_labels = np.array([train_labels] * NUM_TASKS)
task_test_labels = np.array([test_labels] * NUM_TASKS)

# Display shape of the prepared datasets
print(f"Task Train Images Shape: {task_train_images.shape}")  # (10, 60000, 784)
print(f"Task Test Images Shape: {task_test_images.shape}")    # (10, 10000, 784)
print(f"Task Train Labels Shape: {task_train_labels.shape}")  # (10, 60000)
print(f"Task Test Labels Shape: {task_test_labels.shape}")    # (10, 10000)

import numpy as np
import tensorflow as tf

# Number of tasks and other constants
num_tasks_to_run = 10  # Number of sequential tasks
num_epochs_per_task = 20
batch_size = 10000
size_input = 784  # MNIST flattened image size

# Create random permutations for each task
task_permutation = [np.random.permutation(size_input) for _ in range(num_tasks_to_run)]

# Convert raw images to tensors
def imgToTensor(img_array):
    return tf.convert_to_tensor(img_array, dtype=tf.float32)

# Generate permuted datasets for each task
split_train_img = []
split_test_img = []

# First task (original MNIST images, no permutation)
split_train_img.append(imgToTensor(train_images))
split_test_img.append(imgToTensor(test_images))

# Generate permuted tasks
for k in range(num_tasks_to_run):
    split_train_img.append(imgToTensor(train_images[:, task_permutation[k]]))
    split_test_img.append(imgToTensor(test_images[:, task_permutation[k]]))

# Convert labels to TensorFlow tensors
task_train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)
task_test_labels = tf.convert_to_tensor(test_labels, dtype=tf.int32)

# Ensure label tensors match tasks
task_train_labels = tf.stack([task_train_labels] * (num_tasks_to_run + 1))  # +1 for original task
task_test_labels = tf.stack([task_test_labels] * (num_tasks_to_run + 1))

# Check dataset shapes
print(f"Train Image Tasks Shape: {len(split_train_img)} x {split_train_img[0].shape}")  # (11, 60000, 784)
print(f"Test Image Tasks Shape: {len(split_test_img)} x {split_test_img[0].shape}")    # (11, 10000, 784)
print(f"Train Labels Shape: {task_train_labels.shape}")  # (11, 60000)
print(f"Test Labels Shape: {task_test_labels.shape}")    # (11, 10000)

import tensorflow as tf

class MLP(tf.keras.Model):
    def __init__(self, depth=2, dropout_prob=0.0, optimizer_type='Adam'):
        """
        Initialize a Multi-Layer Perceptron (MLP) with configurable depth, dropout, and optimizer.

        Parameters:
        - depth (int): Number of hidden layers (2, 3, or 4).
        - dropout_prob (float): Dropout probability (≤ 0.5).
        - optimizer_type (str): Optimizer choice ('SGD', 'Adam', 'RMSProp').
        """
        super(MLP, self).__init__()

        self.hidden_layers = []
        self.dropout_prob = dropout_prob

        # Input layer
        self.hidden_layers.append(tf.keras.layers.Dense(256, activation='relu'))
        if dropout_prob > 0:
            self.hidden_layers.append(tf.keras.layers.Dropout(dropout_prob))

        # Hidden layers
        for _ in range(depth - 2):  # depth includes input and output layers
            self.hidden_layers.append(tf.keras.layers.Dense(256, activation='relu'))
            if dropout_prob > 0:
                self.hidden_layers.append(tf.keras.layers.Dropout(dropout_prob))

        # Output layer
        self.output_layer = tf.keras.layers.Dense(10)  # 10 classes for MNIST

        # Optimizer selection
        if optimizer_type == "SGD":
            self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
        elif optimizer_type == "RMSProp":
            self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)
        else:
            self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

    def call(self, inputs, training=False):
        """
        Forward pass through the network.

        Parameters:
        - inputs (tensor): Input data.
        - training (bool): Whether the model is in training mode (affects dropout).

        Returns:
        - logits (tensor): Output logits before softmax.
        """
        x = inputs
        for layer in self.hidden_layers:
            x = layer(x, training=training)
        logits = self.output_layer(x)
        return logits

    def compute_loss(self, logits, labels, loss_type="NLL"):
        """
        Compute the loss function.

        Parameters:
        - logits (tensor): Output logits.
        - labels (tensor): True labels.
        - loss_type (str): Loss function ('NLL', 'L1', 'L2', 'L1+L2').

        Returns:
        - loss (tensor): Computed loss value.
        """
        labels_one_hot = tf.one_hot(labels, depth=10)

        if loss_type == "NLL":
            return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))
        elif loss_type == "L1":
            return tf.reduce_mean(tf.abs(tf.nn.softmax(logits) - labels_one_hot))
        elif loss_type == "L2":
            return tf.reduce_mean(tf.square(tf.nn.softmax(logits) - labels_one_hot))
        elif loss_type == "L1+L2":
            l1_loss = tf.reduce_mean(tf.abs(tf.nn.softmax(logits) - labels_one_hot))
            l2_loss = tf.reduce_mean(tf.square(tf.nn.softmax(logits) - labels_one_hot))
            return l1_loss + l2_loss
        else:
            raise ValueError("Invalid loss type. Choose from ['NLL', 'L1', 'L2', 'L1+L2'].")

    def train_step(self, X, labels, loss_type="NLL"):
        """
        Perform one training step.

        Parameters:
        - X (tensor): Input data.
        - labels (tensor): True labels.
        - loss_type (str): Loss function type.

        Returns:
        - loss (tensor): Computed loss value.
        """
        with tf.GradientTape() as tape:
            logits = self.call(X, training=True)
            loss = self.compute_loss(logits, labels, loss_type)

        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        return loss

import tensorflow as tf
import numpy as np

def create_batches(X, y, batch_size=128):
    """
    Generate batches of data for training.

    Parameters:
    - X (tensor): Training images.
    - y (tensor): Training labels.
    - batch_size (int): Number of samples per batch.

    Returns:
    - Generator yielding (batch_X, batch_y).
    """
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)
    return dataset

def train_model(task_train_images, train_labels, model, epochs, loss_type):
    """
    Train the model for a specified number of epochs.
    Returns loss and accuracy for plotting.
    """
    loss_history = []
    accuracy_history = []

    for epoch in range(epochs):
        total_loss = 0
        batches = create_batches(task_train_images, train_labels, batch_size=128)

        for batch_X, batch_y in batches:
            loss = model.train_step(batch_X, batch_y, loss_type)
            total_loss += loss.numpy()

        # Save loss per epoch
        loss_history.append(total_loss)

        # Calculate accuracy after each epoch
        accuracy = test_model(model, task_test_images[0], task_test_labels[0])
        accuracy_history.append(accuracy)

        if (epoch + 1) % 5 == 0 or epoch == 0:  # Print every 5 epochs and always print the first epoch
          print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}")

    return model, loss_history, accuracy_history

def test_model(model, test_images, test_labels):
    """
    Evaluate the model on test data.

    Parameters:
    - model (MLP): The trained model.
    - test_images (tensor): Test images.
    - test_labels (tensor): Test labels.

    Returns:
    - accuracy (float): Model accuracy on the test set.
    """
    logits = model(test_images, training=False)
    predictions = tf.argmax(logits, axis=1)
    test_labels = tf.cast(test_labels, dtype=tf.int64)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, test_labels), tf.float32))
    return accuracy.numpy()

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

# Define parameter ranges
# loss_types = ["NLL", "L1", "L2", "L1+L2"]
# optimizers = ["SGD", "Adam", "RMSProp"]
# depths = [2, 3, 4]
# dropout_rates = [0.2, 0.4, 0.6]

loss_types = ["L1"]
optimizers = ["SGD"]
depths = [4]
dropout_rates = [0.3]

def calculate_ACC(R):
    """
    Compute ACC (Average Accuracy).

    ACC = (1 / T) * Σ RT,i

    Parameters:
    - R (numpy array): Performance matrix (NUM_TASKS x NUM_TASKS).

    Returns:
    - ACC (float): Average final test accuracy.
    """
    T = R.shape[0]
    return np.mean(R[T-1, :T])  # Last row's mean accuracy

def calculate_BWT(R):
    """
    Compute BWT (Backward Transfer).

    BWT = (1 / (T-1)) * Σ (RT,i - Ri,i)

    Parameters:
    - R (numpy array): Performance matrix (NUM_TASKS x NUM_TASKS).

    Returns:
    - BWT (float): Backward Transfer value.
    """
    T = R.shape[0]
    return np.mean(R[T-1, :-1] - np.diag(R)[:-1])

def run_sequential_training(model, loss_type, dropout_prob):
    """
    Train and test the model sequentially on NUM_TASKS.

    Parameters:
    - model (MLP): Initialized MLP model.
    - loss_type (str): Type of loss function used.
    - dropout_prob (float): Dropout rate.

    Returns:
    - R (numpy array): Task performance matrix.
    """
    R = np.zeros((num_tasks_to_run, num_tasks_to_run))  # Resulting Task Matrix

    for task_index in range(num_tasks_to_run):
        epochs = 50 if task_index == 0 else 20

        print(f"\nTraining on Task {task_index + 1}/{num_tasks_to_run}...")
        model, _, _ = train_model(task_train_images[task_index], task_train_labels[task_index], model, epochs, loss_type)

        # Test on all previously seen tasks
        for test_task in range(task_index + 1):
            accuracy = test_model(model, task_test_images[test_task], task_test_labels[test_task])
            R[task_index, test_task] = accuracy
            print(f"Test Accuracy on Task {test_task + 1}: {accuracy:.4f}")

    return R

def experiment():
    """
    Runs experiments for different configurations of loss, optimizer, depth, and dropout.
    Records and logs results.
    """
    results = []

    for loss_type in loss_types:
        for optimizer_type in optimizers:
            for depth in depths:
                for dropout_prob in dropout_rates:
                    print(f"\nRunning Experiment: Loss={loss_type}, Optimizer={optimizer_type}, Depth={depth}, Dropout={dropout_prob}")

                    # Initialize model
                    model = MLP(depth=depth, dropout_prob=dropout_prob, optimizer_type=optimizer_type)

                    # Train and evaluate on sequential tasks
                    R = run_sequential_training(model, loss_type, dropout_prob)

                    # Compute metrics
                    ACC = calculate_ACC(R)
                    BWT = calculate_BWT(R)

                    # Log results
                    results.append((loss_type, optimizer_type, depth, dropout_prob, ACC, BWT))
                    print(f"ACC: {ACC:.4f}, BWT: {BWT:.4f}")

    return results, R

def plot_loss(loss_values, title, epochs):
    """
    Plots loss or accuracy over epochs.

    Parameters:
    - loss_values (list): List of loss or accuracy values.
    - title (str): Title of the plot.
    - epochs (int): Total number of epochs.
    """
    plt.figure(figsize=(10, 5))
    plt.plot(range(epochs), loss_values, label=title)
    plt.xlabel("Epochs")
    plt.ylabel(title)
    plt.legend()
    plt.show()

def plot_experiment(R, experiment_results):
    """
    Plots validation accuracy decrease for all models over time.

    Parameters:
    - experiment_results (list): A list of tuples containing (loss, optimizer, depth, dropout, ACC, BWT).
    """
    num_tasks = 10  # Total tasks trained
    plt.figure(figsize=(12, 6))

    for i in range(R.shape[1]):
        plt.plot(range(1, num_tasks + 1), R[:, i], label=f"Task {i+1}")

    plt.xlabel("Number of Tasks Trained")
    plt.ylabel("Validation Accuracy")
    plt.title("Validation Accuracy Drop Over Time for All Models")
    plt.legend(loc="lower left", fontsize="small", bbox_to_anchor=(1, 0))
    plt.show()

# Run experiments
experiment_results, R = experiment()
plot_experiment(R, experiment_results)